"""
Notebook Export Service
Export data analysis to Jupyter Notebook (.ipynb) or Google Colab
"""

import json
from typing import Dict, Any, List
import base64
import pandas as pd


class NotebookExporter:
    """Export analysis to Jupyter Notebook or Google Colab format"""

    def __init__(self):
        pass

    def create_notebook(self, eda_results: Dict[str, Any], connection_info: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Create a Jupyter Notebook (.ipynb) with the EDA analysis

        Args:
            eda_results: Results from EDA analysis
            connection_info: Optional database connection information

        Returns:
            Dictionary representing a Jupyter Notebook
        """

        cells = []

        # Header cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Analysis Notebook\n",
                "\n",
                "This notebook contains exploratory data analysis generated by AI Data Platform.\n",
                "\n",
                "## Contents:\n",
                "1. Data Loading\n",
                "2. Basic Information\n",
                "3. Statistical Summary\n",
                "4. Missing Values Analysis\n",
                "5. Data Quality Report\n",
                "6. Continue Your Analysis\n"
            ]
        })

        # Imports cell
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline"
            ]
        })

        # Data loading cell
        if connection_info:
            source_type = connection_info.get('source_type', 'file')
            if source_type == 'postgresql':
                cells.append({
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": ["## 1. Data Loading\n", "\n", "Connect to PostgreSQL database:"]
                })
                cells.append({
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "outputs": [],
                    "source": [
                        f"from sqlalchemy import create_engine\n",
                        f"\n",
                        f"# Database connection\n",
                        f"connection_string = 'postgresql://{connection_info.get('username')}:YOUR_PASSWORD@{connection_info.get('host')}:{connection_info.get('port', 5432)}/{connection_info.get('database')}'\n",
                        f"engine = create_engine(connection_string)\n",
                        f"\n",
                        f"# Load data\n",
                        f"query = \"{connection_info.get('query', 'SELECT * FROM table_name LIMIT 1000')}\"\n",
                        f"df = pd.read_sql(query, engine)\n",
                        f"\n",
                        f"print(f'Data loaded: {{df.shape[0]}} rows, {{df.shape[1]}} columns')"
                    ]
                })
            elif source_type == 'bigquery':
                cells.append({
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": ["## 1. Data Loading\n", "\n", "Connect to BigQuery:"]
                })
                cells.append({
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "outputs": [],
                    "source": [
                        "from google.cloud import bigquery\n",
                        "\n",
                        "# Initialize BigQuery client\n",
                        f"client = bigquery.Client(project='{connection_info.get('project_id')}')\n",
                        "\n",
                        f"# Load data\n",
                        f"query = \"\"\"{connection_info.get('query', 'SELECT * FROM `project.dataset.table` LIMIT 1000')}\"\"\"\n",
                        "df = client.query(query).to_dataframe()\n",
                        "\n",
                        "print(f'Data loaded: {df.shape[0]} rows, {df.shape[1]} columns')"
                    ]
                })
            else:
                cells.append({
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": ["## 1. Data Loading\n", "\n", "Upload your data file:"]
                })
                cells.append({
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "outputs": [],
                    "source": [
                        "# Load data from CSV file\n",
                        "# Replace 'your_file.csv' with your actual file path\n",
                        "df = pd.read_csv('your_file.csv')\n",
                        "\n",
                        "print(f'Data loaded: {df.shape[0]} rows, {df.shape[1]} columns')"
                    ]
                })

        # df.head() cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 2. Basic Information\n", "\n", "### First few rows (df.head())"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first 10 rows\n",
                "df.head(10)"
            ]
        })

        # df.shape cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["### Dataset Shape"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Rows: {df.shape[0]:,}')\n",
                "print(f'Columns: {df.shape[1]}')\n",
                "print(f'\\nColumns: {list(df.columns)}')"
            ]
        })

        # df.info() cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["### DataFrame Info (df.info())"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        })

        # df.describe() cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 3. Statistical Summary (df.describe())"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe(include='all')"
            ]
        })

        # Missing values cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 4. Missing Values Analysis (df.isnull().sum())"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missing values count\n",
                "missing_values = df.isnull().sum()\n",
                "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
                "\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_values,\n",
                "    'Percentage': missing_pct\n",
                "}).sort_values('Missing Count', ascending=False)\n",
                "\n",
                "print(missing_df[missing_df['Missing Count'] > 0])\n",
                "\n",
                "# Visualize missing values\n",
                "if missing_df['Missing Count'].sum() > 0:\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    missing_df[missing_df['Missing Count'] > 0]['Percentage'].plot(kind='barh')\n",
                "    plt.xlabel('Percentage Missing')\n",
                "    plt.title('Missing Values by Column')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        })

        # Data types cell
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["### Data Types (df.dtypes)"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.dtypes)\n",
                "\n",
                "# Count by data type\n",
                "print('\\nColumn count by type:')\n",
                "print(df.dtypes.value_counts())"
            ]
        })

        # Correlation matrix for numeric columns
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 5. Correlation Analysis (Numeric Columns)"]
        })
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numeric columns\n",
                "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "\n",
                "if len(numeric_cols) > 1:\n",
                "    # Correlation matrix\n",
                "    corr_matrix = df[numeric_cols].corr()\n",
                "    \n",
                "    # Plot heatmap\n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
                "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
                "    plt.title('Correlation Matrix')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print('Not enough numeric columns for correlation analysis')"
            ]
        })

        # Continue analysis section
        cells.append({
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Continue Your Analysis Below\n",
                "\n",
                "The DataFrame `df` is loaded and ready for further analysis.\n",
                "\n",
                "### Suggested next steps:\n",
                "- Visualize distributions of key variables\n",
                "- Perform feature engineering\n",
                "- Handle missing values\n",
                "- Identify and handle outliers\n",
                "- Build predictive models\n"
            ]
        })

        # Empty code cell for user to continue
        cells.append({
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your analysis code here\n",
                "\n"
            ]
        })

        # Create notebook structure
        notebook = {
            "cells": cells,
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                    },
                    "file_extension": ".py",
                    "mimetype": "text/x-python",
                    "name": "python",
                    "nbconvert_exporter": "python",
                    "pygments_lexer": "ipython3",
                    "version": "3.8.0"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }

        return notebook

    def generate_colab_url(self, notebook: Dict[str, Any]) -> str:
        """
        Generate a Google Colab URL for the notebook

        Args:
            notebook: Jupyter notebook dictionary

        Returns:
            Google Colab URL
        """

        # Convert notebook to JSON string
        notebook_json = json.dumps(notebook)

        # Encode to base64
        notebook_b64 = base64.b64encode(notebook_json.encode()).decode()

        # Create GitHub Gist-style URL (simplified version)
        # In production, you would upload to GitHub Gist or Google Drive
        # For now, we'll return a template URL
        colab_url = f"https://colab.research.google.com/notebook#fileId=new&name=analysis.ipynb"

        return colab_url

    def export_to_ipynb(self, eda_results: Dict[str, Any], connection_info: Dict[str, Any] = None) -> str:
        """
        Export analysis to Jupyter Notebook (.ipynb) format

        Returns:
            JSON string of the notebook
        """

        notebook = self.create_notebook(eda_results, connection_info)
        return json.dumps(notebook, indent=2)
